{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-04T22:05:39.484723700Z",
     "start_time": "2025-01-04T22:05:39.399726900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-04T19:00:27.383483900Z",
     "start_time": "2025-01-04T19:00:27.112497Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_collection import PDF_COLLECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_0 saved\n",
      "file_1 saved\n",
      "file_2 saved\n",
      "file_3 saved\n",
      "file_4 saved\n",
      "file_5 saved\n",
      "file_6 saved\n",
      "file_7 saved\n",
      "file_8 saved\n",
      "file_9 saved\n"
     ]
    }
   ],
   "source": [
    "pdf_collector = PDF_COLLECTOR()\n",
    "pdf_collector.collect_pdfs(\"data_sources/URLs.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-04T19:02:04.906726100Z",
     "start_time": "2025-01-04T19:00:28.232424700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-04 23:05:49 - INFO - Traitement du fichier PDF : PDFs/file_0.pdf\n",
      "2025-01-04 23:05:49 - INFO - Extraction du texte sans les tables pour le fichier : PDFs/file_0.pdf\n",
      "2025-01-04 23:06:20 - INFO - Texte extrait avec succès pour le fichier : PDFs/file_0.pdf\n",
      "2025-01-04 23:06:20 - INFO - Suppression des en-têtes et pieds de page\n",
      "2025-01-04 23:06:20 - INFO - Normalisation des espaces blancs\n",
      "2025-01-04 23:06:20 - INFO - Réparation des lignes cassées\n",
      "2025-01-04 23:06:20 - INFO - Suppression des phrases redondantes\n",
      "2025-01-04 23:06:20 - INFO - Fichier PDF traité avec succès : PDFs/file_0.pdf\n",
      "2025-01-04 23:06:20 - INFO - Traitement du fichier PDF : PDFs/file_1.pdf\n",
      "2025-01-04 23:06:20 - INFO - Extraction du texte sans les tables pour le fichier : PDFs/file_1.pdf\n",
      "2025-01-04 23:07:02 - INFO - Texte extrait avec succès pour le fichier : PDFs/file_1.pdf\n",
      "2025-01-04 23:07:02 - INFO - Suppression des en-têtes et pieds de page\n",
      "2025-01-04 23:07:02 - INFO - Normalisation des espaces blancs\n",
      "2025-01-04 23:07:02 - INFO - Réparation des lignes cassées\n",
      "2025-01-04 23:07:02 - INFO - Suppression des phrases redondantes\n",
      "2025-01-04 23:07:02 - INFO - Fichier PDF traité avec succès : PDFs/file_1.pdf\n",
      "2025-01-04 23:07:02 - INFO - Traitement du fichier PDF : PDFs/file_2.pdf\n",
      "2025-01-04 23:07:02 - INFO - Extraction du texte sans les tables pour le fichier : PDFs/file_2.pdf\n",
      "2025-01-04 23:07:52 - INFO - Texte extrait avec succès pour le fichier : PDFs/file_2.pdf\n",
      "2025-01-04 23:07:52 - INFO - Suppression des en-têtes et pieds de page\n",
      "2025-01-04 23:07:52 - INFO - Normalisation des espaces blancs\n",
      "2025-01-04 23:07:52 - INFO - Réparation des lignes cassées\n",
      "2025-01-04 23:07:52 - INFO - Suppression des phrases redondantes\n",
      "2025-01-04 23:07:52 - INFO - Fichier PDF traité avec succès : PDFs/file_2.pdf\n",
      "2025-01-04 23:07:52 - INFO - Traitement du fichier PDF : PDFs/file_3.pdf\n",
      "2025-01-04 23:07:52 - INFO - Extraction du texte sans les tables pour le fichier : PDFs/file_3.pdf\n",
      "2025-01-04 23:08:48 - INFO - Texte extrait avec succès pour le fichier : PDFs/file_3.pdf\n",
      "2025-01-04 23:08:48 - INFO - Suppression des en-têtes et pieds de page\n",
      "2025-01-04 23:08:48 - INFO - Normalisation des espaces blancs\n",
      "2025-01-04 23:08:48 - INFO - Réparation des lignes cassées\n",
      "2025-01-04 23:08:48 - INFO - Suppression des phrases redondantes\n",
      "2025-01-04 23:08:48 - INFO - Fichier PDF traité avec succès : PDFs/file_3.pdf\n",
      "2025-01-04 23:08:48 - INFO - Traitement du fichier PDF : PDFs/file_4.pdf\n",
      "2025-01-04 23:08:48 - INFO - Extraction du texte sans les tables pour le fichier : PDFs/file_4.pdf\n",
      "2025-01-04 23:09:54 - INFO - Texte extrait avec succès pour le fichier : PDFs/file_4.pdf\n",
      "2025-01-04 23:09:54 - INFO - Suppression des en-têtes et pieds de page\n",
      "2025-01-04 23:09:54 - INFO - Normalisation des espaces blancs\n",
      "2025-01-04 23:09:54 - INFO - Réparation des lignes cassées\n",
      "2025-01-04 23:09:54 - INFO - Suppression des phrases redondantes\n",
      "2025-01-04 23:09:54 - INFO - Fichier PDF traité avec succès : PDFs/file_4.pdf\n",
      "2025-01-04 23:09:54 - INFO - Traitement du fichier PDF : PDFs/file_5.pdf\n",
      "2025-01-04 23:09:54 - INFO - Extraction du texte sans les tables pour le fichier : PDFs/file_5.pdf\n",
      "2025-01-04 23:10:39 - INFO - Texte extrait avec succès pour le fichier : PDFs/file_5.pdf\n",
      "2025-01-04 23:10:39 - INFO - Suppression des en-têtes et pieds de page\n",
      "2025-01-04 23:10:39 - INFO - Normalisation des espaces blancs\n",
      "2025-01-04 23:10:39 - INFO - Réparation des lignes cassées\n",
      "2025-01-04 23:10:39 - INFO - Suppression des phrases redondantes\n",
      "2025-01-04 23:10:39 - INFO - Fichier PDF traité avec succès : PDFs/file_5.pdf\n",
      "2025-01-04 23:10:39 - INFO - Traitement du fichier PDF : PDFs/file_6.pdf\n",
      "2025-01-04 23:10:39 - INFO - Extraction du texte sans les tables pour le fichier : PDFs/file_6.pdf\n",
      "2025-01-04 23:11:57 - INFO - Texte extrait avec succès pour le fichier : PDFs/file_6.pdf\n",
      "2025-01-04 23:11:57 - INFO - Suppression des en-têtes et pieds de page\n",
      "2025-01-04 23:11:57 - INFO - Normalisation des espaces blancs\n",
      "2025-01-04 23:11:57 - INFO - Réparation des lignes cassées\n",
      "2025-01-04 23:11:57 - INFO - Suppression des phrases redondantes\n",
      "2025-01-04 23:11:57 - INFO - Fichier PDF traité avec succès : PDFs/file_6.pdf\n",
      "2025-01-04 23:11:57 - INFO - Traitement du fichier PDF : PDFs/file_7.pdf\n",
      "2025-01-04 23:11:57 - INFO - Extraction du texte sans les tables pour le fichier : PDFs/file_7.pdf\n",
      "2025-01-04 23:12:54 - INFO - Texte extrait avec succès pour le fichier : PDFs/file_7.pdf\n",
      "2025-01-04 23:12:54 - INFO - Suppression des en-têtes et pieds de page\n",
      "2025-01-04 23:12:54 - INFO - Normalisation des espaces blancs\n",
      "2025-01-04 23:12:54 - INFO - Réparation des lignes cassées\n",
      "2025-01-04 23:12:54 - INFO - Suppression des phrases redondantes\n",
      "2025-01-04 23:12:54 - INFO - Fichier PDF traité avec succès : PDFs/file_7.pdf\n",
      "2025-01-04 23:12:54 - INFO - Traitement du fichier PDF : PDFs/file_8.pdf\n",
      "2025-01-04 23:12:54 - INFO - Extraction du texte sans les tables pour le fichier : PDFs/file_8.pdf\n",
      "2025-01-04 23:13:50 - INFO - Texte extrait avec succès pour le fichier : PDFs/file_8.pdf\n",
      "2025-01-04 23:13:50 - INFO - Suppression des en-têtes et pieds de page\n",
      "2025-01-04 23:13:50 - INFO - Normalisation des espaces blancs\n",
      "2025-01-04 23:13:50 - INFO - Réparation des lignes cassées\n",
      "2025-01-04 23:13:50 - INFO - Suppression des phrases redondantes\n",
      "2025-01-04 23:13:50 - INFO - Fichier PDF traité avec succès : PDFs/file_8.pdf\n",
      "2025-01-04 23:13:50 - INFO - Traitement du fichier PDF : PDFs/file_9.pdf\n",
      "2025-01-04 23:13:50 - INFO - Extraction du texte sans les tables pour le fichier : PDFs/file_9.pdf\n",
      "2025-01-04 23:15:12 - INFO - Texte extrait avec succès pour le fichier : PDFs/file_9.pdf\n",
      "2025-01-04 23:15:12 - INFO - Suppression des en-têtes et pieds de page\n",
      "2025-01-04 23:15:12 - INFO - Normalisation des espaces blancs\n",
      "2025-01-04 23:15:12 - INFO - Réparation des lignes cassées\n",
      "2025-01-04 23:15:12 - INFO - Suppression des phrases redondantes\n",
      "2025-01-04 23:15:12 - INFO - Fichier PDF traité avec succès : PDFs/file_9.pdf\n"
     ]
    }
   ],
   "source": [
    "from data_cleaning import DataCleaner\n",
    "import os\n",
    "\n",
    "files_paths = [f\"PDFs/{file_path}\" for file_path in os.listdir(\"PDFs\")]\n",
    "dataCleaner = DataCleaner(files_paths)\n",
    "\n",
    "cleaned_text = dataCleaner.clean_text()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-04T22:15:12.339304100Z",
     "start_time": "2025-01-04T22:05:45.798508500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3812045\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-04T22:22:22.765124400Z",
     "start_time": "2025-01-04T22:22:22.128120300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kmail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kmail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kmail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_cleaning import DataPreprocessor\n",
    "\n",
    "preprocessor = DataPreprocessor(language=\"english\")\n",
    "preprocessed_text = preprocessor.preprocess(cleaned_text, remove_stopwords=True, lemmatize=True)\n",
    "print(\"Cleaned Text:\", preprocessed_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-04T22:23:13.161011700Z",
     "start_time": "2025-01-04T22:22:59.095374900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "3005969"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-04T22:23:17.391143100Z",
     "start_time": "2025-01-04T22:23:17.277144300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "with open(\"test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(preprocessed_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-04T22:23:39.092025600Z",
     "start_time": "2025-01-04T22:23:38.770905900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-04 20:54:12 - INFO - PyTorch version 2.5.1 available.\n",
      "2025-01-04 20:54:12 - INFO - PyTorch version 2.5.1 available.\n",
      "2025-01-04 20:54:12 - INFO - PyTorch version 2.5.1 available.\n",
      "2025-01-04 20:54:12 - INFO - PyTorch version 2.5.1 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (509027 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/796 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce3eb2e5014b46529d0069c5a028c4ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/199 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cf5532749584fe2ab648c360210dc04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmail\\OneDrive\\Bureau\\Work_Directory\\Projet Wael\\venv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodel_training\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtraining\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fine_tune_llm\n\u001B[1;32m----> 3\u001B[0m \u001B[43mfine_tune_llm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreprocessed_text\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\Work_Directory\\Projet Wael\\model_training\\training.py:104\u001B[0m, in \u001B[0;36mfine_tune_llm\u001B[1;34m(text, model_name, output_dir, epochs, eval_split)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;66;03m# Initialize and run trainer\u001B[39;00m\n\u001B[0;32m     97\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     98\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     99\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m    100\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mtokenized_train_dataset,\n\u001B[0;32m    101\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39mtokenized_eval_dataset,\n\u001B[0;32m    102\u001B[0m )\n\u001B[1;32m--> 104\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;66;03m# Save fine-tuned model and tokenizer\u001B[39;00m\n\u001B[0;32m    107\u001B[0m model\u001B[38;5;241m.\u001B[39msave_pretrained(output_dir)\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\Work_Directory\\Projet Wael\\venv\\lib\\site-packages\\transformers\\trainer.py:2164\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2162\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2165\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2168\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2169\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\Work_Directory\\Projet Wael\\venv\\lib\\site-packages\\transformers\\trainer.py:2524\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2517\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2518\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[0;32m   2519\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   2520\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[0;32m   2521\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[0;32m   2522\u001B[0m )\n\u001B[0;32m   2523\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m-> 2524\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   2527\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[0;32m   2528\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m   2529\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[0;32m   2530\u001B[0m ):\n\u001B[0;32m   2531\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[0;32m   2532\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\Work_Directory\\Projet Wael\\venv\\lib\\site-packages\\transformers\\trainer.py:3654\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[1;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[0;32m   3651\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m   3653\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[1;32m-> 3654\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3656\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[0;32m   3657\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   3658\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   3659\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   3660\u001B[0m ):\n",
      "File \u001B[1;32m~\\OneDrive\\Bureau\\Work_Directory\\Projet Wael\\venv\\lib\\site-packages\\transformers\\trainer.py:3729\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[0;32m   3727\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3728\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m outputs:\n\u001B[1;32m-> 3729\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   3730\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3731\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(outputs\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. For reference, the inputs it received are \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(inputs\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3732\u001B[0m         )\n\u001B[0;32m   3733\u001B[0m     \u001B[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001B[39;00m\n\u001B[0;32m   3734\u001B[0m     loss \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "from model_training.training import fine_tune_llm\n",
    "\n",
    "fine_tune_llm(text=preprocessed_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-04T19:55:16.708767900Z",
     "start_time": "2025-01-04T19:54:11.243202500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "fine_tune_llm(text=texts)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 984,
     "referenced_widgets": [
      "0a75e4a12ca54d6f9972845becc0fe18",
      "588ccc0b0b264b808199c3947c16fe0d",
      "f2b63d6a551e42e8b177b01f0c68f099",
      "e4a623384cce4f22a5b5142788a29e77",
      "d5359133770e44148b39e3b4454adaf9",
      "7d2254def4f2476b817d944998195a44",
      "db62c71d4bad454dad36f9594e434a98",
      "063bcd972e3b46a5b1653d737ca27271",
      "71babe8623aa43d3bb9f3f4552c821c9",
      "a45d93a2bf09438f8d692d84c9c1407f",
      "229b1f1868e346c78789a73851ed2d2f"
     ]
    },
    "id": "1tuAC_SIneEW",
    "outputId": "8dc83042-c658-42da-be54-00cee2c4e7f8"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (509027 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/796 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a75e4a12ca54d6f9972845becc0fe18"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 05:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.699300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.585600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(GPT2LMHeadModel(\n",
       "   (transformer): GPT2Model(\n",
       "     (wte): Embedding(50257, 768)\n",
       "     (wpe): Embedding(1024, 768)\n",
       "     (drop): Dropout(p=0.1, inplace=False)\n",
       "     (h): ModuleList(\n",
       "       (0-11): 12 x GPT2Block(\n",
       "         (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): GPT2SdpaAttention(\n",
       "           (c_attn): Conv1D(nf=2304, nx=768)\n",
       "           (c_proj): Conv1D(nf=768, nx=768)\n",
       "           (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "           (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): GPT2MLP(\n",
       "           (c_fc): Conv1D(nf=3072, nx=768)\n",
       "           (c_proj): Conv1D(nf=768, nx=3072)\n",
       "           (act): NewGELUActivation()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       " ),\n",
       " GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " }\n",
       " ))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  }
 ]
}
